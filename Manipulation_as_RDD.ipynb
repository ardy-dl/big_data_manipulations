{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWWKQqFKYG6t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "236cbf58-5547-4d11-da5d-686deddcc10e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hcanceled\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext"
      ],
      "metadata": {
        "id": "a4hjTHJ6v04V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "ER39UpDZm7Uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = pyspark.SparkContext(appName=\"performanceFactors\")"
      ],
      "metadata": {
        "id": "AXzFtAd9oLAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "factors = sc.textFile(\"/content/StudentPerformanceFactors.csv\")"
      ],
      "metadata": {
        "id": "NdBxRymYxG8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "factors.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mden6khhxXVD",
        "outputId": "04c6f373-a094-4c1c-da71-d88567a79c9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(factors))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PklbmKwTxtp5",
        "outputId": "d732a3bf-1a86-4ccb-f862-ae31742f49d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.rdd.RDD'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(factors.getNumPartitions())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-USsox-3x5WS",
        "outputId": "2017110a-1016-4b85-9fcf-b239be2d54cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext(appName=\"performanceFactors\")\n",
        "factorsrdd = sc.textFile(\"/content/StudentPerformanceFactors.csv\")"
      ],
      "metadata": {
        "id": "M1wUAH2RyB1p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da947735-3990-476f-ad38-59d2b2494b52"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "header = factorsrdd.first()  # header\n",
        "datardd = factorsrdd.filter(lambda row: row != header).map(lambda line: line.split(\",\"))  # rdd\n",
        "\n",
        "column_names = header.split(\",\")  # split headers\n",
        "column_index = {column_names[i].strip(): i for i in range(len(column_names))}  # dictionary for each column\n",
        "\n",
        "# index of specific columns\n",
        "exam_score_column = column_index['Exam_Score']\n",
        "previous_exam_column = column_index['Previous_Scores']\n",
        "sleep_column = column_index['Sleep_Hours']\n",
        "hrs_studied_column = column_index['Hours_Studied']\n",
        "income_column = column_index['Family_Income']\n",
        "Access_to_Resources = column_index['Access_to_Resources']\n",
        "Teacher_column = column_index['Teacher_Quality']\n",
        "\n",
        "# Filter students who passed both exams\n",
        "passed_both_filtered = datardd.filter(lambda row: (\n",
        "    int(row[previous_exam_column].strip()) >= 70 and\n",
        "    int(row[exam_score_column].strip()) >= 70\n",
        "))\n",
        "\n",
        "# Filter students who passed the previous exam but not the recent one\n",
        "passed_previous_filtered = datardd.filter(lambda row: (\n",
        "    int(row[previous_exam_column].strip()) >= 70 and\n",
        "    int(row[exam_score_column].strip()) < 70\n",
        "))\n",
        "\n",
        "# Combine selected columns\n",
        "both_exam = passed_both_filtered.map(lambda row: (\n",
        "    int(row[hrs_studied_column].strip()),\n",
        "    (int(row[previous_exam_column].strip()),\n",
        "     row[income_column].strip(),\n",
        "     row[sleep_column].strip() + \" hrs\",\n",
        "     int(row[exam_score_column].strip()))\n",
        "))\n",
        "\n",
        "previous_exam = passed_previous_filtered.map(lambda row: (\n",
        "    row[income_column].strip(),\n",
        "    (int(row[previous_exam_column].strip()),\n",
        "     int(row[hrs_studied_column].strip()),\n",
        "     row[sleep_column].strip() + \" hrs\",\n",
        "     int(row[exam_score_column].strip()))\n",
        "))\n",
        "\n",
        "# Group by Family Income and sort by Previous Exam Score within each group\n",
        "groupby_income = previous_exam.groupByKey()\n",
        "sortedby_income = groupby_income.mapValues(lambda records: sorted(records, key=lambda x: x[1]))\n",
        "\n",
        "# Collect and print the results\n",
        "groupsorted_data = sortedby_income.collect()\n",
        "\n",
        "#for group_income, records in groupsorted_data:\n",
        "#    print(\"Family Income: \", group_income)\n",
        "#    for record in records:\n",
        "#        print(f\"  Hours_Studied: {record[1]}, Previous_Score: {record[0]}, Sleep_Hours: {record[2]}, Recent_Exam_Score: {record[3]}\")\n"
      ],
      "metadata": {
        "id": "3n6Kq9spzN1I"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MenuQdAr8S2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "zgEoFxfs6AsP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import col, when, count\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Student Performance\").getOrCreate()\n",
        "\n",
        "df = datardd.map(lambda row: Row(\n",
        "    Hours_Studied=int(row[hrs_studied_column].strip()),\n",
        "    Previous_Scores=int(row[previous_exam_column].strip()),\n",
        "    Family_Income=row[income_column].strip(),\n",
        "    Sleep_Hours=int(row[sleep_column].strip()),\n",
        "    Exam_Score=int(row[exam_score_column].strip()),\n",
        "    Resources=row[Access_to_Resources].strip(),\n",
        "    Teacher_Quality=row[Teacher_column].strip()\n",
        ")).toDF()\n",
        "\n",
        "# select\n",
        "selected_df = df.select(\"Hours_Studied\", \"Previous_Scores\", \"Family_Income\", \"Sleep_Hours\", \"Exam_Score\", \"Resources\", \"Teacher_Quality\")\n",
        "# filter\n",
        "filtered_df = selected_df.filter(selected_df[\"Previous_Scores\"] >= 70)\n",
        "\n",
        "# adding score range\n",
        "score_range_df = filtered_df.withColumn(\n",
        "    \"Score_Range\",\n",
        "    when((col(\"Previous_Scores\") >= 90), \"Line of 9 and above\")\n",
        "    .when((col(\"Previous_Scores\") >= 80) & (col(\"Previous_Scores\") < 90), \"Line of 8\")\n",
        "    .when((col(\"Previous_Scores\") >= 70) & (col(\"Previous_Scores\") < 80), \"Line of 7\")\n",
        ")\n",
        "\n",
        "grouped_score_income_df = score_range_df.groupBy(\"Resources\", \"Score_Range\").agg(count(\"*\").alias(\"Total_Students\"))\n",
        "\n",
        "# final results\n",
        "grouped_score_income_df.orderBy(\"Resources\", \"Score_Range\").show()"
      ],
      "metadata": {
        "id": "kyQk41er6O_W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c3d09f0-12f2-404a-a70c-af8dc3830804"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------------+--------------+\n",
            "|Resources|        Score_Range|Total_Students|\n",
            "+---------+-------------------+--------------+\n",
            "|     High|          Line of 7|           395|\n",
            "|     High|          Line of 8|           412|\n",
            "|     High|Line of 9 and above|           427|\n",
            "|      Low|          Line of 7|           271|\n",
            "|      Low|          Line of 8|           258|\n",
            "|      Low|Line of 9 and above|           243|\n",
            "|   Medium|          Line of 7|           660|\n",
            "|   Medium|          Line of 8|           654|\n",
            "|   Medium|Line of 9 and above|           721|\n",
            "+---------+-------------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cqn-P-13AM2l"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}